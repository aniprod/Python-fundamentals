# Python-fundamentals
My first data engineering project.

Step 1: Download a dataset from kaggle

Step 2: Programatically upload it in a GCS bucket

Step 3: Create a BigQuery Dataset

Step 4: Run transformations on the data using dbt

Step 5: Automate the pipeline using Airflow and Docker for contanerization


Airflow resources from Panos:
#https://dev.to/markbdsouza/apache-airflow-for-beginners-16o
 
#https://theaisummer.com/apache-airflow-tutorial/
 
#https://blog.adnansiddiqi.me/using-apache-airflow-etl-to-fetch-and-analyze-btc-data/
 
#https://www.franciscoyira.com/post/data-pipelines-cloud-intro-airflow-docker/

#installing-airflow-on-the-windows-subsystem-for-linux